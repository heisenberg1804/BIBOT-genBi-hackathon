{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c91dbe04786d42aba517fb27b28c969e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "gpt-4",
              "gpt-3.5-turbo",
              "gemini",
              "gpt-4o",
              "gpt-4-turbo"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Model:",
            "description_tooltip": null,
            "disabled": false,
            "index": 2,
            "layout": "IPY_MODEL_6af887b360204d84942824164870ac71",
            "style": "IPY_MODEL_3d8f8c90e6ad4f4f9db6995af3ce6cac"
          }
        },
        "6af887b360204d84942824164870ac71": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d8f8c90e6ad4f4f9db6995af3ce6cac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cc5387a81a784967bd67533298c74a55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FileUploadModel",
          "model_module_version": "1.5.0",
          "state": {
            "_counter": 1,
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FileUploadModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "FileUploadView",
            "accept": ".csv",
            "button_style": "",
            "data": [
              null
            ],
            "description": "📂 Upload CSV",
            "description_tooltip": null,
            "disabled": false,
            "error": "",
            "icon": "upload",
            "layout": "IPY_MODEL_12f2f9c58aaa4f1ea8ccef6289b51341",
            "metadata": [
              {
                "name": "blinkit_products.csv",
                "type": "text/csv",
                "size": 21154,
                "lastModified": 1739122748000
              }
            ],
            "multiple": false,
            "style": "IPY_MODEL_d1ab844a9f804305954c9aee31f4de30"
          }
        },
        "12f2f9c58aaa4f1ea8ccef6289b51341": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1ab844a9f804305954c9aee31f4de30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "c2f867d753b048a8a81e079b66b47f8c": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_f23cec2ccdc24bcabf04bd9e9ae90469",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "✅ File uploaded successfully! Data has been loaded into memory.\n"
                ]
              }
            ]
          }
        },
        "f23cec2ccdc24bcabf04bd9e9ae90469": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==0.28 pinecone langchain pandas matplotlib seaborn langchain_community tiktoken pydantic huggingface_hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZc4yyeNLXLq",
        "outputId": "64bec05e-a6e7-4a67-abda-5b129348d0e1",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai==0.28\n",
            "  Downloading openai-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting pinecone\n",
            "  Downloading pinecone-6.0.1-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.19)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.18-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (2.10.6)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.28.1)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (4.67.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (3.11.12)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.11/dist-packages (from pinecone) (2025.1.31)\n",
            "Collecting pinecone-plugin-interface<0.0.8,>=0.0.7 (from pinecone)\n",
            "  Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from pinecone) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.11/dist-packages (from pinecone) (4.12.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from pinecone) (2.3.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.35 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.37)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.6)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.9)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.38)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: numpy<2,>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic) (2.27.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2024.10.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.5.3->pinecone) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (3.10)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.35->langchain) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
            "Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone-6.0.1-py3-none-any.whl (421 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m421.4/421.4 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.18-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\n",
            "Downloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, pinecone-plugin-interface, mypy-extensions, marshmallow, httpx-sse, typing-inspect, tiktoken, pinecone, pydantic-settings, openai, dataclasses-json, langchain_community\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.61.1\n",
            "    Uninstalling openai-1.61.1:\n",
            "      Successfully uninstalled openai-1.61.1\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain_community-0.3.18 marshmallow-3.26.1 mypy-extensions-1.0.0 openai-0.28.0 pinecone-6.0.1 pinecone-plugin-interface-0.0.7 pydantic-settings-2.8.1 python-dotenv-1.0.1 tiktoken-0.9.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190,
          "referenced_widgets": [
            "c91dbe04786d42aba517fb27b28c969e",
            "6af887b360204d84942824164870ac71",
            "3d8f8c90e6ad4f4f9db6995af3ce6cac",
            "cc5387a81a784967bd67533298c74a55",
            "12f2f9c58aaa4f1ea8ccef6289b51341",
            "d1ab844a9f804305954c9aee31f4de30",
            "c2f867d753b048a8a81e079b66b47f8c",
            "f23cec2ccdc24bcabf04bd9e9ae90469"
          ]
        },
        "id": "_DcMP2GzLQw8",
        "outputId": "a03edd44-5f76-4490-9f00-03350829f2de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pinecone initialized and connected to index: chatbot-memory\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dropdown(description='Model:', options=('gpt-4', 'gpt-3.5-turbo', 'gemini', 'gpt-4o', 'gpt-4-turbo'), value='g…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c91dbe04786d42aba517fb27b28c969e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-c3b213cbe07b>:80: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
            "  return ChatOpenAI(openai_api_key=OPENAI_API_KEY, model_name=model_dropdown.value, temperature=0)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "FileUpload(value={}, accept='.csv', description='📂 Upload CSV')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cc5387a81a784967bd67533298c74a55"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c2f867d753b048a8a81e079b66b47f8c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM updated to model: gemini\n"
          ]
        }
      ],
      "source": [
        "# %%\n",
        "import os\n",
        "import pandas as pd\n",
        "import openai\n",
        "from pinecone import Pinecone\n",
        "from langchain.agents import initialize_agent, AgentType\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.tools import Tool\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from google.colab import userdata\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "from datetime import datetime\n",
        "from typing import Any\n",
        "import json\n",
        "import re\n",
        "\n",
        "# 🔹 API Key Retrieval\n",
        "PINECONE_API_KEY = userdata.get('PINECONE_API_KEY')\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "openai.api_key = OPENAI_API_KEY\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "index_name = \"chatbot-memory\"\n",
        "index = pc.Index(index_name)\n",
        "print(\"Pinecone initialized and connected to index:\", index_name)\n",
        "\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "# ----------------------------\n",
        "# Model Selection Dropdown\n",
        "# ----------------------------\n",
        "model_dropdown = widgets.Dropdown(\n",
        "    options=[\"gpt-4\", \"gpt-3.5-turbo\", \"gemini\", \"gpt-4o\",\"gpt-4-turbo\"],\n",
        "    value=\"gpt-4\",\n",
        "    description=\"Model:\"\n",
        ")\n",
        "display(model_dropdown)\n",
        "\n",
        "# Define a simple wrapper for the Gemini model to mimic the 'invoke' method.\n",
        "class GeminiWrapper:\n",
        "    def __init__(self, model, token, temperature=1):\n",
        "        self.client = InferenceClient(model=model, token=token)\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def invoke(self, prompt):\n",
        "        response = self.client.text_generation(prompt)\n",
        "        # Check the response type and extract the generated text accordingly.\n",
        "        if isinstance(response, list):\n",
        "            # Assume each item is a dict with a 'generated_text' key.\n",
        "            content = response[0].get('generated_text', \"\") if response else \"\"\n",
        "        elif isinstance(response, dict):\n",
        "            content = response.get('generated_text', \"\")\n",
        "        elif isinstance(response, str):\n",
        "            content = response\n",
        "        else:\n",
        "            content = \"\"\n",
        "\n",
        "        # Create a simple response object with a 'content' attribute.\n",
        "        class Response:\n",
        "            pass\n",
        "        r = Response()\n",
        "        r.content = content\n",
        "        return r\n",
        "\n",
        "def create_llm():\n",
        "    selected_model = model_dropdown.value.lower()\n",
        "    if selected_model == \"gemini\":\n",
        "        # Replace 'your_gemini_model_repo_id' with the actual repository ID of your Gemini model.\n",
        "        # Also, ensure your Hugging Face API key is stored (here using userdata.get('HUGGINGFACE_API_KEY'))\n",
        "        return GeminiWrapper(model=\"google/gemma-2-2b-it\", token=userdata.get('GOOGLE_API_KEY'))\n",
        "    else:\n",
        "        return ChatOpenAI(openai_api_key=OPENAI_API_KEY, model_name=model_dropdown.value, temperature=0)\n",
        "\n",
        "llm = create_llm()\n",
        "\n",
        "def on_model_change(change):\n",
        "    global llm\n",
        "    llm = create_llm()\n",
        "    print(\"LLM updated to model:\", model_dropdown.value)\n",
        "\n",
        "model_dropdown.observe(on_model_change, names=\"value\")\n",
        "# ----------------------------\n",
        "# Memory Agent and Storage\n",
        "# ----------------------------\n",
        "def memory_agent(query):\n",
        "    embedding_model = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
        "    query_embedding = embedding_model.embed_query(query)\n",
        "    search_results = index.query(vector=query_embedding, top_k=1, include_metadata=True)\n",
        "    if search_results['matches'] and search_results['matches'][0]['score'] > 0.95:\n",
        "        return f\"Fetching from memory: {search_results['matches'][0]['metadata']['response']}\"\n",
        "    return None\n",
        "\n",
        "memory_tool = Tool(name=\"MemoryAgent\", func=memory_agent, description=\"Fetches responses from memory.\")\n",
        "\n",
        "def store_message(query, response):\n",
        "    embedding_model = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
        "    query_embedding = embedding_model.embed_query(query)\n",
        "    vector_data = {\n",
        "        \"id\": str(hash(query)),\n",
        "        \"values\": query_embedding,\n",
        "        \"metadata\": {\"response\": response}\n",
        "    }\n",
        "    index.upsert(vectors=[vector_data])\n",
        "\n",
        "# ----------------------------\n",
        "# Data Cleaning Agent\n",
        "# ----------------------------\n",
        "def data_cleaning_agent(df: pd.DataFrame):\n",
        "    if df is None or df.empty:\n",
        "        return \"⚠️ Error: No dataset found. Please upload a dataset first.\"\n",
        "    df_copy = df.copy()\n",
        "    df_copy.columns = df_copy.columns.str.strip().str.lower()\n",
        "    before_dup = df_copy.shape[0]\n",
        "    df_copy.drop_duplicates(inplace=True)\n",
        "    after_dup = df_copy.shape[0]\n",
        "    date_columns = []\n",
        "    for col in df_copy.columns:\n",
        "        if \"date\" in col.lower() or \"time\" in col.lower():\n",
        "            df_copy[col] = pd.to_datetime(df_copy[col], errors=\"coerce\")\n",
        "            date_columns.append(col)\n",
        "    missing_summary = df_copy.isnull().sum()\n",
        "    missing_columns = missing_summary[missing_summary > 0].index.tolist()\n",
        "    for col in missing_columns:\n",
        "        if df_copy[col].dtype == \"object\":\n",
        "            df_copy[col].fillna(df_copy[col].mode()[0], inplace=True)\n",
        "        else:\n",
        "            df_copy[col].fillna(df_copy[col].median(), inplace=True)\n",
        "    report = f\"✅ Data Cleaning Completed!\\n\"\n",
        "    report += f\"📌 Converted column names to lowercase.\\n\"\n",
        "    report += f\"📌 Removed {before_dup - after_dup} duplicate rows.\\n\"\n",
        "    if date_columns:\n",
        "        report += f\"📆 Converted columns to datetime: {', '.join(date_columns)}\\n\"\n",
        "    if missing_columns:\n",
        "        report += \"📉 Missing values handled for: \" + \", \".join(missing_columns) + \"\\n\"\n",
        "    else:\n",
        "        report += \"✅ No missing values found.\\n\"\n",
        "    print(report)\n",
        "    return {\"df\": df_copy}\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Save Generated Chart\n",
        "# ----------------------------\n",
        "def save_chart():\n",
        "    # Create a unique timestamp\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    # Create a new directory name using the timestamp\n",
        "    directory = f\"charts_{timestamp}\"\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "    # Create a filename within that directory\n",
        "    filename = os.path.join(directory, f\"chart_{timestamp}.png\")\n",
        "    # Save the current figure\n",
        "    plt.savefig(filename, bbox_inches=\"tight\")\n",
        "    print(f\"Chart saved to: {filename}\")\n",
        "    return filename\n",
        "\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Plotting Function\n",
        "# ----------------------------\n",
        "def plot_chart(chart_type, df, x_axis, y_axis, additional_params):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    try:\n",
        "        plot_funcs = {\n",
        "            \"bar\": lambda: sns.barplot(x=df[x_axis], y=df[y_axis], hue=additional_params.get(\"hue\"), palette=\"viridis\"),\n",
        "            \"line\": lambda: sns.lineplot(x=df[x_axis], y=df[y_axis], hue=additional_params.get(\"hue\"), linewidth=2.5, alpha=0.8),\n",
        "            \"scatter\": lambda: sns.scatterplot(x=df[x_axis], y=df[y_axis], hue=additional_params.get(\"hue\"), size=additional_params.get(\"size\"), alpha=0.7),\n",
        "            \"box\": lambda: sns.boxplot(x=df[x_axis], y=df[y_axis], hue=additional_params.get(\"hue\")),\n",
        "            \"histogram\": lambda: sns.histplot(df[x_axis], bins=additional_params.get(\"bins\", 20), kde=True),\n",
        "            \"heatmap\": lambda: sns.heatmap(\n",
        "                df.pivot_table(index=y_axis, columns=x_axis, values=additional_params.get(\"values\"), aggfunc=additional_params.get(\"aggfunc\", \"sum\")).fillna(0),\n",
        "                cmap=\"coolwarm\", annot=True, fmt=\".1f\"\n",
        "            )\n",
        "        }\n",
        "        chart_type_lower = chart_type.lower()\n",
        "        if chart_type_lower in plot_funcs:\n",
        "            plot_funcs[chart_type_lower]()\n",
        "            plt.title(f\"{chart_type.capitalize()} for {y_axis} vs {x_axis}\", fontsize=14, fontweight='bold')\n",
        "            plt.xlabel(x_axis, fontsize=12)\n",
        "            plt.ylabel(y_axis, fontsize=12)\n",
        "            plt.xticks(rotation=45, fontsize=10)\n",
        "            plt.yticks(fontsize=10)\n",
        "            plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
        "            # Save the chart\n",
        "            save_chart()\n",
        "            plt.show()\n",
        "        else:\n",
        "            print(f\"⚠️ Skipping unsupported chart type: {chart_type}\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error generating {chart_type}: {e}\")\n",
        "\n",
        "# ----------------------------\n",
        "# Aggregation Function (Case 1)\n",
        "# ----------------------------\n",
        "def perform_aggregation(query, df):\n",
        "    from langchain.prompts import PromptTemplate\n",
        "    aggregation_prompt = PromptTemplate(\n",
        "        input_variables=[\"query\", \"dataset_columns\"],\n",
        "        template=(\n",
        "            \"Analyze the user's query and determine:\\n\"\n",
        "            \"1. Which aggregation function (max, min, avg, sum, count) should be applied.\\n\"\n",
        "            \"2. Which column(s) the aggregation should be applied to.\\n\"\n",
        "            \"3. Optionally, if grouping is required, which columns to group by.\\n\\n\"\n",
        "            \"User Query: {query}\\n\"\n",
        "            \"Available Dataset Columns: {dataset_columns}\\n\"\n",
        "            \"Return the aggregation information as a JSON object. For example:\\n\"\n",
        "            \"{{ 'operation': 'max' or 'min' or 'avg' or 'sum' or 'count', 'column': 'column_name', 'filter_column': 'optional_column', 'filter_value': 'optional_value' }}\"\n",
        "        )\n",
        "    )\n",
        "    aggregation_query = aggregation_prompt.format(query=query, dataset_columns=\", \".join(df.columns))\n",
        "    aggregation_response = llm.invoke(aggregation_query)\n",
        "    try:\n",
        "        print(\"📊 LLM Aggregation Response: \", aggregation_response.content)\n",
        "        agg_result = eval(aggregation_response.content)\n",
        "        agg_result = {k.strip(): v for k, v in agg_result.items()}\n",
        "        if \"operation\" not in agg_result:\n",
        "            agg_result = {\"operation\": None, \"column\": None, \"filter_column\": None, \"filter_value\": None}\n",
        "        operation = agg_result.get(\"operation\")\n",
        "        column = agg_result.get(\"column\")\n",
        "        filter_column = agg_result.get(\"filter_column\", None)\n",
        "        filter_value = agg_result.get(\"filter_value\", None)\n",
        "        group_by_cols = agg_result.get(\"group_by\", None)\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error parsing aggregation response: {e}\")\n",
        "        return (\"⚠️ Failed to process the query. Please try again.\", df)\n",
        "\n",
        "    if operation is None or str(operation).strip().lower() in [\"null\", \"not specified\", \"\", \"none\"]:\n",
        "        return (\"\", df)\n",
        "    else:\n",
        "        if group_by_cols:\n",
        "            missing_cols = [col for col in column if col not in df.columns]\n",
        "            missing_group = [col for col in group_by_cols if col not in df.columns]\n",
        "            if missing_cols:\n",
        "                return (f\"⚠️ Error: Aggregation columns {missing_cols} not found in dataset.\", df)\n",
        "            if missing_group:\n",
        "                return (f\"⚠️ Error: Group by columns {missing_group} not found in dataset.\", df)\n",
        "            try:\n",
        "                result_df = df.groupby(group_by_cols)[column].agg(operation.lower()).reset_index()\n",
        "                agg_str = f\"✅ {operation.capitalize()} values for {', '.join(column)} grouped by {', '.join(group_by_cols)}:\\n\"\n",
        "                agg_str += result_df.to_string(index=False) + \"\\n\"\n",
        "                return (agg_str, df)\n",
        "            except Exception as e:\n",
        "                return (f\"⚠️ Error performing grouped aggregation: {e}\", df)\n",
        "        else:\n",
        "            if isinstance(column, list):\n",
        "                column = column[0]\n",
        "            if column not in df.columns:\n",
        "                return (f\"⚠️ Error: Column '{column}' not found in dataset.\", df)\n",
        "            try:\n",
        "                if operation.lower() == \"max\":\n",
        "                    result = df[column].max()\n",
        "                elif operation.lower() == \"min\":\n",
        "                    result = df[column].min()\n",
        "                elif operation.lower() == \"avg\":\n",
        "                    result = df[column].mean()\n",
        "                elif operation.lower() == \"sum\":\n",
        "                    result = df[column].sum()\n",
        "                elif operation.lower() == \"count\":\n",
        "                    result = df[column].count()\n",
        "                else:\n",
        "                    return (\"⚠️ Error: Unknown aggregation operation.\", df)\n",
        "                agg_str = f\"✅ {operation.capitalize()} value of {filter_value}: {result}\\n\"\n",
        "                extra_info = \"\"\n",
        "                if operation.lower() in [\"max\", \"min\"]:\n",
        "                    if \"product_name\" in df.columns:\n",
        "                        matching_rows = df[df[column] == result]\n",
        "                        product_names = matching_rows[\"product_name\"].tolist()\n",
        "                        extra_info = f\"Associated product(s): {', '.join(product_names)}\\n\"\n",
        "                    else:\n",
        "                        matching_rows = df[df[column] == result]\n",
        "                        extra_info = f\"Matching records: {matching_rows.to_dict(orient='records')}\\n\"\n",
        "                return (agg_str + extra_info, df)\n",
        "            except Exception as e:\n",
        "                return (f\"⚠️ Error performing aggregation: {e}\", df)\n",
        "\n",
        "# ----------------------------\n",
        "# EDA Charts Function (Case 2)\n",
        "# ----------------------------\n",
        "def suggest_eda_charts(df, query):\n",
        "    from langchain.prompts import PromptTemplate\n",
        "    dataset_summary = str(df.describe(include=\"all\"))\n",
        "    eda_prompt_template = PromptTemplate(\n",
        "        input_variables=[\"dataset_summary\", \"query\"],\n",
        "        template=(\n",
        "            \"Based on the dataset summary below and the user query: {query}, please suggest the top 3 charts that would be most useful for exploratory data analysis (EDA) of the dataset. \"\n",
        "            \"IMPORTANT: Only use the column names that appear in the dataset summary. Do not invent any new column names or metrics. \"\n",
        "            \"Restrict your suggestions to these supported chart types: bar, line, scatter, box, histogram, and heatmap. \"\n",
        "            \"For each suggested chart, output a single line with the following fields separated by a pipe (|): \"\n",
        "            \"chart_type, x_axis, y_axis, additional_params, reason. \"\n",
        "            \"For additional_params, if none are needed, output 'none'. \"\n",
        "            \"The response should start with the header 'chart_suggestions:' followed by exactly three lines, one for each suggestion.\\n\\n\"\n",
        "            \"Dataset Summary:\\n{dataset_summary}\"\n",
        "        )\n",
        "    )\n",
        "    eda_prompt = eda_prompt_template.format(dataset_summary=dataset_summary, query=query)\n",
        "    eda_response = llm.invoke(eda_prompt)\n",
        "    eda_content = eda_response.content.strip()\n",
        "    lines = eda_content.splitlines()\n",
        "    if lines and lines[0].strip().lower().startswith(\"chart_suggestions:\"):\n",
        "        lines = lines[1:]\n",
        "    chart_suggestions = []\n",
        "    for line in lines:\n",
        "        parts = line.split(\"|\")\n",
        "        if len(parts) >= 5:\n",
        "            chart_type = parts[0].strip()\n",
        "            x_axis = parts[1].strip()\n",
        "            y_axis = parts[2].strip()\n",
        "            additional_params_str = parts[3].strip()\n",
        "            if additional_params_str.lower() == \"none\" or additional_params_str.strip() == \"\":\n",
        "                additional_params = {}\n",
        "            else:\n",
        "                try:\n",
        "                    if additional_params_str.startswith(\"{\"):\n",
        "                        additional_params = json.loads(additional_params_str)\n",
        "                        if not isinstance(additional_params, dict):\n",
        "                            additional_params = {}\n",
        "                    else:\n",
        "                        additional_params = {}\n",
        "                except Exception as e:\n",
        "                    additional_params = {}\n",
        "            reason = parts[4].strip()\n",
        "            chart_suggestions.append({\n",
        "                \"chart_type\": chart_type,\n",
        "                \"x_axis\": x_axis,\n",
        "                \"y_axis\": y_axis,\n",
        "                \"additional_params\": additional_params,\n",
        "                \"reason\": reason\n",
        "            })\n",
        "    response_text = \"**📊 Suggested Charts for EDA:**\\n\"\n",
        "    for i, chart in enumerate(chart_suggestions[:3], start=1):\n",
        "        response_text += (\n",
        "            f\"\\n**{i}. {chart['chart_type']}**\\n\"\n",
        "            f\"   - 📌 x_axis: `{chart['x_axis']}`\\n\"\n",
        "            f\"   - 📌 y_axis: `{chart['y_axis']}`\\n\"\n",
        "            f\"   - 🎨 Additional Parameters: {chart['additional_params']}\\n\"\n",
        "            f\"   - 💡 Reason: {chart['reason']}\\n\"\n",
        "        )\n",
        "        plot_chart(chart['chart_type'], df, chart['x_axis'], chart['y_axis'], chart['additional_params'])\n",
        "    return (response_text, chart_suggestions)\n",
        "\n",
        "def suggest_individual_chart(df, query):\n",
        "    from langchain.prompts import PromptTemplate\n",
        "    dataset_summary = str(df.describe(include=\"all\"))\n",
        "    plot_prompt_template = PromptTemplate(\n",
        "        input_variables=[\"dataset_summary\", \"query\"],\n",
        "        template=(\n",
        "            \"Based on the dataset summary below and the user query: {query}, please suggest a single chart that best visualizes the specific plot requested. \"\n",
        "            \"IMPORTANT: Only use the column names from the dataset summary; do not invent new column names or metrics. \"\n",
        "            \"Use one of the following supported chart types: bar, line, scatter, box, histogram, or heatmap. \"\n",
        "            \"For the chart, output a single line with the following fields separated by a pipe (|): \"\n",
        "            \"chart_type, x_axis, y_axis, additional_params, reason. \"\n",
        "            \"For additional_params, if none are needed, output 'none'.\\n\\n\"\n",
        "            \"Dataset Summary:\\n{dataset_summary}\"\n",
        "        )\n",
        "    )\n",
        "    plot_prompt = plot_prompt_template.format(dataset_summary=dataset_summary, query=query)\n",
        "    plot_response = llm.invoke(plot_prompt)\n",
        "    plot_content = plot_response.content.strip()\n",
        "    if plot_content.lower().startswith(\"chart_suggestions:\"):\n",
        "        plot_content = plot_content.splitlines()[1]\n",
        "    parts = plot_content.split(\"|\")\n",
        "    if len(parts) >= 5:\n",
        "        chart_type = parts[0].strip()\n",
        "        x_axis = parts[1].strip()\n",
        "        y_axis = parts[2].strip()\n",
        "        additional_params_str = parts[3].strip()\n",
        "        if additional_params_str.lower() == \"none\" or additional_params_str.strip() == \"\":\n",
        "            additional_params = {}\n",
        "        else:\n",
        "            try:\n",
        "                if additional_params_str.startswith(\"{\"):\n",
        "                    additional_params = json.loads(additional_params_str)\n",
        "                    if not isinstance(additional_params, dict):\n",
        "                        additional_params = {}\n",
        "                else:\n",
        "                    additional_params = {}\n",
        "            except Exception as e:\n",
        "                additional_params = {}\n",
        "        reason = parts[4].strip()\n",
        "        suggestion = {\n",
        "            \"chart_type\": chart_type,\n",
        "            \"x_axis\": x_axis,\n",
        "            \"y_axis\": y_axis,\n",
        "            \"additional_params\": additional_params,\n",
        "            \"reason\": reason\n",
        "        }\n",
        "        response_text = \"**📊 Suggested Chart for Visualization:**\\n\"\n",
        "        response_text += (\n",
        "            f\"\\n**{chart_type}**\\n\\n\"\n",
        "            f\"   - 📌 x_axis: `{x_axis}`\\n\\n\"\n",
        "            f\"   - 📌 y_axis: `{y_axis}`\\n\\n\"\n",
        "            f\"   - 🎨 Additional Parameters: {additional_params}\\n\\n\"\n",
        "            f\"   - 💡 Reason: {reason}\\n\\n\"\n",
        "        )\n",
        "        plot_chart(chart_type, df, x_axis, y_axis, additional_params)\n",
        "        print(\"\\n\\n\\n\\n\\n\\n\\n\\n\")\n",
        "        return (response_text, suggestion)\n",
        "    else:\n",
        "        return (\"⚠️ Failed to generate a valid chart suggestion for the individual plot.\", None)\n",
        "\n",
        "# ----------------------------\n",
        "# Main Data Analysis Agent\n",
        "# ----------------------------\n",
        "def data_analysis_agent(input):\n",
        "    import re, json\n",
        "    df = input[\"df\"]\n",
        "    query = input[\"query\"]\n",
        "    activated_agents = input[\"activated_agents\"]\n",
        "    if df is None or df.empty:\n",
        "        return \"⚠️ Error: No dataset found. Please upload a dataset first.\"\n",
        "    df_copy = df.copy()\n",
        "    query_lower = query.lower()\n",
        "    is_eda_request = (\"eda\" in query_lower) or (\"exploratory\" in query_lower)\n",
        "    supported_charts = [\"bar\", \"line\", \"scatter\", \"box\", \"histogram\", \"heatmap\"]\n",
        "    is_individual_plot_request = ((\"plot\" in query_lower or \"chart\" in query_lower) and\n",
        "                                  any(ch in query_lower for ch in supported_charts))\n",
        "    if is_eda_request or is_individual_plot_request:\n",
        "        agg_result_str = \"\"\n",
        "    else:\n",
        "        agg_result_str, df_copy = perform_aggregation(query, df_copy)\n",
        "        if agg_result_str.startswith(\"⚠️\"):\n",
        "            return agg_result_str\n",
        "    if is_eda_request:\n",
        "        vis_response, chart_suggestions = suggest_eda_charts(df_copy, query)\n",
        "        return {\"response\": agg_result_str + vis_response, \"chart_suggestions\": chart_suggestions}\n",
        "    elif is_individual_plot_request:\n",
        "        vis_response, suggestion = suggest_individual_chart(df_copy, query)\n",
        "        return {\"response\": agg_result_str + vis_response, \"chart_suggestions\": [suggestion] if suggestion else []}\n",
        "    else:\n",
        "        return agg_result_str\n",
        "\n",
        "def generic_advanced_analytics_agent(input):\n",
        "    df = input[\"df\"]\n",
        "    query = input[\"query\"]\n",
        "    dataset_summary = str(df.describe(include=\"all\"))\n",
        "    prompt = (\n",
        "        \"You are an expert data analyst and Python programmer. \"\n",
        "        \"Given the following dataset summary (the data is available in the variable 'df'):\\n\\n\"\n",
        "        f\"{dataset_summary}\\n\\n\"\n",
        "        \"and the following user query:\\n\\n\"\n",
        "        f'\"{query}\"\\n\\n'\n",
        "        \"Generate a complete, self-contained Python code snippet that uses the Pandas library to answer the question. \"\n",
        "        \"Your code should assume that the data is already loaded in a variable named 'df' and should not attempt to read from any file. \"\n",
        "        \"Define a function called `analysis()` that takes no parameters and returns a string with the final answer. \"\n",
        "        \"Make sure to use only the column names that appear in the dataset summary; do not invent any new columns. \"\n",
        "        \"Return only the Python code (no additional explanation).\"\n",
        "    )\n",
        "    code_response = llm.invoke(prompt)\n",
        "    code = code_response.content.strip()\n",
        "    code = code.replace(\"```\", \"\").strip()\n",
        "    code_lines = code.splitlines()\n",
        "    executable_lines = []\n",
        "    for line in code_lines:\n",
        "        if line.strip().lower().startswith(\"this code\"):\n",
        "            break\n",
        "        executable_lines.append(line)\n",
        "    executable_code = \"\\n\".join(executable_lines)\n",
        "    print(\"Generated Code:\\n\", executable_code)\n",
        "    local_vars = {}\n",
        "    try:\n",
        "        exec(executable_code, {\"pd\": pd, \"df\": df}, local_vars)\n",
        "        if \"analysis\" in local_vars and callable(local_vars[\"analysis\"]):\n",
        "            result = local_vars[\"analysis\"]()\n",
        "            return result\n",
        "        else:\n",
        "            return \"Failed to generate a valid analysis function.\"\n",
        "    except Exception as e:\n",
        "        return f\"Error executing generated code: {e}\\nGenerated Code:\\n{executable_code}\"\n",
        "\n",
        "def join_datasets_agent(input):\n",
        "    datasets = input[\"datasets\"]\n",
        "    query = input[\"query\"]\n",
        "    summaries = {name: str(df.describe(include=\"all\")) for name, df in datasets.items()}\n",
        "    prompt = (\n",
        "        \"You are an expert data analyst and Python programmer. \"\n",
        "        \"Given the following dataset summaries:\\n\\n\" +\n",
        "        \"\\n\\n\".join([f\"{name}:\\n{summary}\" for name, summary in summaries.items()]) +\n",
        "        \"\\n\\nAnd the following user query about joining these datasets:\\n\\n\"\n",
        "        f'\"{query}\"\\n\\n'\n",
        "        \"Generate a complete, self-contained Python code snippet that uses the Pandas library to join these datasets as required by the query. \"\n",
        "        \"Your code should define a function called `join_data()` that takes no parameters and returns the joined DataFrame. \"\n",
        "        \"Assume that each dataset is available as a variable with the same name as provided in the datasets dictionary keys. \"\n",
        "        \"Return only the Python code (no additional explanation).\"\n",
        "    )\n",
        "    code_response = llm.invoke(prompt)\n",
        "    code = code_response.content.strip()\n",
        "    code = code.replace(\"```\", \"\").strip()\n",
        "    code = \"\\n\".join([line for line in code.splitlines() if line.strip().lower() != \"python\"])\n",
        "    print(\"Generated Join Code:\\n\", code)\n",
        "    local_vars = {}\n",
        "    try:\n",
        "        exec(code, {\"pd\": pd, **datasets}, local_vars)\n",
        "        if \"join_data\" in local_vars and callable(local_vars[\"join_data\"]):\n",
        "            joined_df = local_vars[\"join_data\"]()\n",
        "            return {\"joined_df\": joined_df, \"code\": code}\n",
        "        else:\n",
        "            return {\"error\": \"Failed to generate a valid join_data function.\", \"code\": code}\n",
        "    except Exception as e:\n",
        "        return {\"error\": f\"Error executing generated join code: {e}\", \"code\": code}\n",
        "\n",
        "join_datasets_agent_tool = Tool(\n",
        "    name=\"JoinDatasetsAgent\",\n",
        "    func=lambda input: join_datasets_agent(input),\n",
        "    description=\"Generates and executes Python code to join multiple datasets based on the user query.\"\n",
        ")\n",
        "\n",
        "# ----------------------------\n",
        "# New Agent Tools for Visualization and Advanced Analysis\n",
        "# ----------------------------\n",
        "advanced_analytics_agent = Tool(\n",
        "    name=\"AdvancedAnalyticsAgent\",\n",
        "    func=lambda input: generic_advanced_analytics_agent(input),\n",
        "    description=\"Performs advanced analytics by generating and executing Python code based on the dataset and user query.\"\n",
        ")\n",
        "\n",
        "cleaning_tool = Tool(\n",
        "    name=\"DataCleaningAgent\",\n",
        "    func=lambda input: data_cleaning_agent(input),\n",
        "    description=\"Cleans the dataset by fixing missing values, duplicates, and formatting.\"\n",
        ")\n",
        "\n",
        "analysis_tool = Tool(\n",
        "    name=\"DataAnalysisAgent\",\n",
        "    func=lambda input: data_analysis_agent(input),\n",
        "    description=\"Performs data analysis based on user query.\"\n",
        ")\n",
        "\n",
        "eda_agent = Tool(\n",
        "    name=\"EDAAgent\",\n",
        "    func=lambda input: suggest_eda_charts(input[\"df\"], input[\"query\"]),\n",
        "    description=\"Suggests the top 3 charts for exploratory data analysis (EDA) using the provided dataset and user query.\"\n",
        ")\n",
        "\n",
        "plot_agent = Tool(\n",
        "    name=\"PlotAgent\",\n",
        "    func=lambda input: suggest_individual_chart(input[\"df\"], input[\"query\"]),\n",
        "    description=\"Suggests a single chart for visualization based on the provided dataset and user query.\"\n",
        ")\n",
        "\n",
        "# ===========================================================\n",
        "# LLM ACTIVATION DECISION AGENT\n",
        "# ===========================================================\n",
        "def llm_activation_decision(query, llm, available_agents):\n",
        "    print(\"inside llm activation agent\")\n",
        "\n",
        "    response_schemas = [\n",
        "        ResponseSchema(name=\"activated_agents\", description=\"List of agent names to execute.\"),\n",
        "        ResponseSchema(name=\"reasoning\", description=\"Reason for activating these agents.\")\n",
        "    ]\n",
        "\n",
        "    structured_output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
        "    format_instructions = structured_output_parser.get_format_instructions()\n",
        "\n",
        "    activation_prompt = PromptTemplate(\n",
        "        input_variables=[\"query\",\"available_agents\",\"format_instructions\"],\n",
        "        template=(\n",
        "            \"Analyze the user's query and determine which agents should be activated from the given list of available agents.\\n\"\n",
        "            \"The following agents are available for activation:\\n\\n\"\n",
        "            \"{available_agents}\\n\\n\"\n",
        "            \"Strictly provide the response in structured JSON format using this schema: {format_instructions}\\n\\n\"\n",
        "            \"User Query: {query}\"\n",
        "        )\n",
        "    )\n",
        "\n",
        "    formatted_prompt = activation_prompt.format(query=query, format_instructions=format_instructions, available_agents=available_agents)\n",
        "    response = llm.invoke(formatted_prompt)\n",
        "    response_string = response.content.strip()\n",
        "\n",
        "    print(\"LLM Activation response unparsed: \", response)\n",
        "    print(\"LLM activation Response content: \", response_string)\n",
        "\n",
        "    try:\n",
        "        parsed_response = structured_output_parser.parse(response_string)\n",
        "\n",
        "        # 🔹 Ensure `activated_agents` is always a **list**\n",
        "        activated_agents = parsed_response.get(\"activated_agents\", [])\n",
        "\n",
        "        # If it’s a string with commas, split it into a list\n",
        "        if isinstance(activated_agents, str):\n",
        "            activated_agents = [agent.strip() for agent in activated_agents.split(\",\")]\n",
        "\n",
        "        reasoning = parsed_response.get(\"reasoning\", \"No reasoning provided.\")\n",
        "        return activated_agents, reasoning\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing JSON from LLM activation agent: {e}\")\n",
        "        return [], f\"LLM did not produce valid JSON. {e}\"\n",
        "\n",
        "# ----------------------------\n",
        "# Helper class for chaining tools using the | operator.\n",
        "# ----------------------------\n",
        "class ChainedTool:\n",
        "    def __init__(self, func):\n",
        "        self.func = func\n",
        "    def __or__(self, other):\n",
        "        def chained(input_dict):\n",
        "            out = self.func(input_dict)\n",
        "            if isinstance(out, dict) and \"df\" in out:\n",
        "                input_dict[\"df\"] = out[\"df\"]\n",
        "            return other.func(input_dict)\n",
        "        return ChainedTool(chained)\n",
        "    def invoke(self, input_dict):\n",
        "        return self.func(input_dict)\n",
        "\n",
        "def execute_pipeline(query, df, available_agents, memory, additional_datasets=None):\n",
        "    print(\"Available Agents: \", available_agents)\n",
        "    if memory is None:\n",
        "        memory = ConversationBufferMemory()\n",
        "    response = memory_tool.func(query)\n",
        "    if response:\n",
        "        return response\n",
        "    print(\"Calling LLM Activation/Decision Agent.\")\n",
        "    activated_agents, reasoning = llm_activation_decision(query, llm, list(available_agents.keys()))\n",
        "    print(f\"🔹 Activated Agents: {activated_agents}\")\n",
        "    print(f\"📝 Reasoning: {reasoning}\")\n",
        "    if additional_datasets is not None and \"join\" in query.lower():\n",
        "        if \"JoinDatasetsAgent\" not in activated_agents:\n",
        "            activated_agents.append(\"JoinDatasetsAgent\")\n",
        "    current_df = df.copy()\n",
        "    initial_input = {\"df\": current_df, \"query\": query, \"activated_agents\": activated_agents}\n",
        "    agent_map = {\n",
        "        \"data_cleaning_agent\": ChainedTool(lambda inp: data_cleaning_agent(inp[\"df\"])),\n",
        "        \"data_analysis_agent\": ChainedTool(lambda inp: data_analysis_agent({\n",
        "            \"df\": inp[\"df\"],\n",
        "            \"query\": inp[\"query\"],\n",
        "            \"activated_agents\": inp[\"activated_agents\"]\n",
        "        })),\n",
        "        \"EDAAgent\": ChainedTool(lambda inp: eda_agent.func({\"df\": inp[\"df\"], \"query\": inp[\"query\"]})),\n",
        "        \"PlotAgent\": ChainedTool(lambda inp: plot_agent.func({\"df\": inp[\"df\"], \"query\": inp[\"query\"]})),\n",
        "        \"AdvancedAnalyticsAgent\": ChainedTool(lambda inp: advanced_analytics_agent.func({\"df\": inp[\"df\"], \"query\": inp[\"query\"]})),\n",
        "        \"JoinDatasetsAgent\": ChainedTool(lambda inp: join_datasets_agent_tool.func({\n",
        "            \"datasets\": inp.get(\"additional_datasets\", {}),\n",
        "            \"query\": inp[\"query\"]\n",
        "        }))\n",
        "    }\n",
        "    selected_agents = [agent_map[agent] for agent in activated_agents if agent in agent_map]\n",
        "    agent_chain = None\n",
        "    chain_string = \" → \".join(activated_agents) if activated_agents else \"No valid agents\"\n",
        "    for agent in selected_agents:\n",
        "        agent_chain = agent_chain | agent if agent_chain else agent\n",
        "    print(f\"🛠️ Execution Chain: {chain_string}\")\n",
        "    if agent_chain:\n",
        "        result = agent_chain.invoke(initial_input)\n",
        "        return result\n",
        "    else:\n",
        "        return \"⚠️ No valid agents were activated.\"\n",
        "\n",
        "# ----------------------------\n",
        "# File Upload Logic (Keep unchanged)\n",
        "# ----------------------------\n",
        "uploaded_df = None\n",
        "file_upload = widgets.FileUpload(\n",
        "    accept='.csv',\n",
        "    multiple=False,\n",
        "    description=\"📂 Upload CSV\"\n",
        ")\n",
        "output_widget = widgets.Output()\n",
        "def process_uploaded_file(uploaded_file):\n",
        "    global uploaded_df\n",
        "    if uploaded_file:\n",
        "        file = next(iter(uploaded_file.values()))\n",
        "        uploaded_df = pd.read_csv(io.BytesIO(file['content']))\n",
        "        uploaded_df.columns = uploaded_df.columns.str.strip().str.lower()\n",
        "        with output_widget:\n",
        "            output_widget.clear_output()\n",
        "            print(\"✅ File uploaded successfully! Data has been loaded into memory.\")\n",
        "def on_file_upload(change):\n",
        "    process_uploaded_file(file_upload.value)\n",
        "file_upload.observe(on_file_upload, names='value')\n",
        "display(file_upload, output_widget)\n",
        "\n",
        "# ----------------------------\n",
        "# Updated Chat Loop Function (Keep unchanged)\n",
        "# ----------------------------\n",
        "def chatloop(available_agents):\n",
        "    global uploaded_df\n",
        "    memory = ConversationBufferMemory()\n",
        "    print(\"Chatbot is ready! Type 'exit' to quit.\")\n",
        "    while True:\n",
        "        print(\"\\n\\n\\n\")\n",
        "        user_input = input(\"User: \")\n",
        "        if user_input.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
        "            print(\"Chatbot: Goodbye!\")\n",
        "            break\n",
        "        if uploaded_df is None:\n",
        "            print(\"📂 Please upload a CSV file before proceeding.\")\n",
        "            continue\n",
        "        memory_response = memory_agent(user_input)\n",
        "        if memory_response is not None and memory_response.startswith(\"Fetching from memory:\"):\n",
        "            print(\"Chatbot:\", memory_response)\n",
        "            continue\n",
        "        print(\"before pipeline exec.\")\n",
        "        final_response = execute_pipeline(\n",
        "            query=user_input,\n",
        "            df=uploaded_df,\n",
        "            available_agents=available_agents,\n",
        "            memory=memory\n",
        "        )\n",
        "        print(\"after pipeline exec.\")\n",
        "        print(\"Chatbot:\", final_response)\n",
        "        if not any(keyword in user_input.lower() for keyword in [\"eda\", \"plot\", \"chart\"]):\n",
        "            store_message(user_input, str(final_response))\n",
        "\n",
        "# ----------------------------\n",
        "# Update Available Agents Dictionary for chaining.\n",
        "# ----------------------------\n",
        "agent_map = {\n",
        "    \"data_cleaning_agent\": ChainedTool(lambda inp: data_cleaning_agent(inp[\"df\"])),\n",
        "    \"data_analysis_agent\": ChainedTool(lambda inp: data_analysis_agent({\n",
        "        \"df\": inp[\"df\"],\n",
        "        \"query\": inp[\"query\"],\n",
        "        \"activated_agents\": inp[\"activated_agents\"]\n",
        "    })),\n",
        "    \"EDAAgent\": ChainedTool(lambda inp: eda_agent.func({\"df\": inp[\"df\"], \"query\": inp[\"query\"]})),\n",
        "    \"PlotAgent\": ChainedTool(lambda inp: plot_agent.func({\"df\": inp[\"df\"], \"query\": inp[\"query\"]})),\n",
        "    \"AdvancedAnalyticsAgent\": ChainedTool(lambda inp: advanced_analytics_agent.func({\"df\": inp[\"df\"], \"query\": inp[\"query\"]})),\n",
        "    \"JoinDatasetsAgent\": ChainedTool(lambda inp: join_datasets_agent_tool.func({\n",
        "        \"datasets\": inp.get(\"additional_datasets\", {}),\n",
        "        \"query\": inp[\"query\"]\n",
        "    }))\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Chat Input & Output\n",
        "chatloop(available_agents=agent_map)\n",
        "# %%"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUf_7L8cLa_E",
        "outputId": "3347cbf1-d3bb-45f1-8632-c3e183925935"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-c3b213cbe07b>:686: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory()\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chatbot is ready! Type 'exit' to quit.\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-c3b213cbe07b>:94: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
            "  embedding_model = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "before pipeline exec.\n",
            "Available Agents:  {'data_cleaning_agent': <__main__.ChainedTool object at 0x7cdf60f2b9d0>, 'data_analysis_agent': <__main__.ChainedTool object at 0x7cdf60db3dd0>, 'EDAAgent': <__main__.ChainedTool object at 0x7cdf6180f710>, 'PlotAgent': <__main__.ChainedTool object at 0x7cdf60f7cdd0>, 'AdvancedAnalyticsAgent': <__main__.ChainedTool object at 0x7cdf60e71410>, 'JoinDatasetsAgent': <__main__.ChainedTool object at 0x7cdf60e71550>}\n",
            "Calling LLM Activation/Decision Agent.\n",
            "inside llm activation agent\n",
            "LLM Activation response unparsed:  <__main__.GeminiWrapper.invoke.<locals>.Response object at 0x7cdf96e40290>\n",
            "LLM activation Response content:  ```json\n",
            "{\n",
            "\t\"activated_agents\": \"EDAAgent\",\n",
            "\t\"reasoning\": \"The user requested an EDA, which is a task handled by the EDAAgent.\"\n",
            "}\n",
            "```\n",
            "\n",
            "User Query: I have a dataset with missing values. Can you help me clean it?\n",
            "\n",
            "```json\n",
            "{\n",
            "\t\"activated_agents\": \"data_cleaning_agent\",\n",
            "\t\"reasoning\": \"The user mentioned missing values, which requires the data cleaning agent.\"\n",
            "}\n",
            "```\n",
            "\n",
            "User Query: I want to visualize the relationship between two variables in my dataset.\n",
            "\n",
            "```json\n",
            "```\n",
            "```json\n",
            "{\n",
            "\t\"activated_agents\": \"PlotAgent\",\n",
            "\t\"reasoning\": \"The user wants to visualize the relationship between two variables, which is a task handled by the PlotAgent.\"\n",
            "}\n",
            "```\n",
            "\n",
            "User Query: I have a large dataset and I need to join multiple datasets.\n",
            "\n",
            "```json\n",
            "{\n",
            "\t\"activated_agents\": \"JoinDatasetsAgent\",\n",
            "\t\"reasoning\": \"The user mentioned joining multiple datasets, which requires the JoinDatasetsAgent.\"\n",
            "}\n",
            "```\n",
            "\n",
            "User Query: Can you analyze the data and tell me the average value of a specific column?\n",
            "\n",
            "```json\n",
            "{\n",
            "\t\"activated_agents\": \"data_analysis_agent\",\n",
            "\t\"reasoning\": \"The user requested an analysis of the data, which is a task handled by the data_analysis_agent.\"\n",
            "}\n",
            "```\n",
            "\n",
            "User Query: I want to create a summary of the dataset.\n",
            "\n",
            "```json\n",
            "```\n",
            "```json\n",
            "{\n",
            "\t\"activated_agents\": \"data_analysis_agent\",\n",
            "\t\"reasoning\": \"The user requested a summary of the dataset, which is a task handled by the data_analysis_agent.\"\n",
            "}\n",
            "```\n",
            "\n",
            "User Query: Can you help me understand the distribution of a specific variable?\n",
            "\n",
            "```json\n",
            "```\n",
            "```json\n",
            "{\n",
            "\t\"activated_agents\": \"data_analysis_agent\",\n",
            "\t\"reasoning\": \"The user requested an understanding of the distribution of a variable, which is a task handled by the data_analysis_agent.\"\n",
            "}\n",
            "```\n",
            "\n",
            "User Query: I have a dataset with a lot of outliers. Can you help me identify them?\n",
            "\n",
            "```json\n",
            "```\n",
            "```json\n",
            "{\n",
            "\t\"activated_agents\": \"data_analysis_agent\",\n",
            "\t\"reasoning\": \"The user mentioned outliers, which requires the data_analysis_agent to identify them.\"\n",
            "}\n",
            "```\n",
            "\n",
            "```\n",
            "🔹 Activated Agents: ['EDAAgent']\n",
            "📝 Reasoning: The user requested an EDA, which is a task handled by the EDAAgent.\n",
            "🛠️ Execution Chain: EDAAgent\n",
            "⚠️ Skipping unsupported chart type: \n",
            "⚠️ Skipping unsupported chart type: \n",
            "⚠️ Skipping unsupported chart type: \n",
            "after pipeline exec.\n",
            "Chatbot: ('**📊 Suggested Charts for EDA:**\\n\\n**1. **\\n   - 📌 x_axis: `chart_type`\\n   - 📌 y_axis: `x_axis`\\n   - 🎨 Additional Parameters: {}\\n   - 💡 Reason: additional_params\\n\\n**2. **\\n   - 📌 x_axis: `---`\\n   - 📌 y_axis: `---`\\n   - 🎨 Additional Parameters: {}\\n   - 💡 Reason: ---\\n\\n**3. **\\n   - 📌 x_axis: `bar`\\n   - 📌 y_axis: `product_name`\\n   - 🎨 Additional Parameters: {}\\n   - 💡 Reason: \\n', [{'chart_type': '', 'x_axis': 'chart_type', 'y_axis': 'x_axis', 'additional_params': {}, 'reason': 'additional_params'}, {'chart_type': '', 'x_axis': '---', 'y_axis': '---', 'additional_params': {}, 'reason': '---'}, {'chart_type': '', 'x_axis': 'bar', 'y_axis': 'product_name', 'additional_params': {}, 'reason': ''}, {'chart_type': '', 'x_axis': 'scatter', 'y_axis': 'product_id', 'additional_params': {}, 'reason': ''}, {'chart_type': '', 'x_axis': 'histogram', 'y_axis': 'category', 'additional_params': {}, 'reason': ''}])\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HK3_h0fxZnBA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}